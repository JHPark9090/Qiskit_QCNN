{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e459b024-955f-484e-ac1b-c0006521c9d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping qiskit-terra as it is not installed.\u001b[0m\u001b[33m\n",
      "Found existing installation: qiskit-machine-learning 0.8.2\n",
      "Uninstalling qiskit-machine-learning-0.8.2:\n",
      "  Successfully uninstalled qiskit-machine-learning-0.8.2\n",
      "Found existing installation: qiskit-ibm-runtime 0.37.0\n",
      "Uninstalling qiskit-ibm-runtime-0.37.0:\n",
      "  Successfully uninstalled qiskit-ibm-runtime-0.37.0\n",
      "Found existing installation: qiskit-aer 0.12.1\n",
      "Uninstalling qiskit-aer-0.12.1:\n",
      "  Successfully uninstalled qiskit-aer-0.12.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qiskit-machine-learning\n",
      "  Using cached qiskit_machine_learning-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qiskit-ibm-runtime\n",
      "  Using cached qiskit_ibm_runtime-0.37.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting qiskit-aer\n",
      "  Using cached qiskit_aer-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: qiskit>=1.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-machine-learning) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.4 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-machine-learning) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-machine-learning) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-machine-learning) (5.9.7)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-machine-learning) (1.3.2)\n",
      "Requirement already satisfied: setuptools>=40.1 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-machine-learning) (69.0.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-machine-learning) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-ibm-runtime) (2.32.3)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-ibm-runtime) (1.3.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-ibm-runtime) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-ibm-runtime) (2.8.2)\n",
      "Requirement already satisfied: websocket-client>=1.5.1 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-ibm-runtime) (1.7.0)\n",
      "Requirement already satisfied: ibm-platform-services>=0.22.6 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-ibm-runtime) (0.59.0)\n",
      "Requirement already satisfied: pydantic<2.10,>=2.5.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit-ibm-runtime) (2.9.2)\n",
      "Collecting qiskit>=1.0 (from qiskit-machine-learning)\n",
      "  Using cached qiskit-2.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit-ibm-runtime) (23.2)\n",
      "Requirement already satisfied: ibm-cloud-sdk-core<4.0.0,>=3.22.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime) (3.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from pydantic<2.10,>=2.5.0->qiskit-ibm-runtime) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from pydantic<2.10,>=2.5.0->qiskit-ibm-runtime) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from pydantic<2.10,>=2.5.0->qiskit-ibm-runtime) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit-ibm-runtime) (1.16.0)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit>=1.0->qiskit-machine-learning) (0.15.1)\n",
      "Requirement already satisfied: sympy>=1.3 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit>=1.0->qiskit-machine-learning) (1.13.1)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit>=1.0->qiskit-machine-learning) (5.3.0)\n",
      "Requirement already satisfied: symengine<0.14,>=0.11 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit>=1.0->qiskit-machine-learning) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from requests>=2.19->qiskit-ibm-runtime) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from requests>=2.19->qiskit-ibm-runtime) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from requests>=2.19->qiskit-ibm-runtime) (2023.11.17)\n",
      "Requirement already satisfied: cryptography>=1.3 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime) (41.0.7)\n",
      "Requirement already satisfied: pyspnego>=0.4.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime) (0.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from scikit-learn>=1.2->qiskit-machine-learning) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from scikit-learn>=1.2->qiskit-machine-learning) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.22.0->ibm-platform-services>=0.22.6->qiskit-ibm-runtime) (2.9.0)\n",
      "Requirement already satisfied: pbr>=2.0.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit>=1.0->qiskit-machine-learning) (6.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from sympy>=1.3->qiskit>=1.0->qiskit-machine-learning) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime) (2.21)\n",
      "Using cached qiskit_machine_learning-0.8.2-py3-none-any.whl (231 kB)\n",
      "Using cached qiskit_ibm_runtime-0.37.0-py3-none-any.whl (3.1 MB)\n",
      "Using cached qiskit_aer-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Using cached qiskit-2.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Installing collected packages: qiskit, qiskit-machine-learning, qiskit-aer, qiskit-ibm-runtime\n",
      "  Attempting uninstall: qiskit\n",
      "    Found existing installation: qiskit 1.1.2\n",
      "    Uninstalling qiskit-1.1.2:\n",
      "      Successfully uninstalled qiskit-1.1.2\n",
      "Successfully installed qiskit-2.0.0 qiskit-aer-0.17.0 qiskit-ibm-runtime-0.37.0 qiskit-machine-learning-0.8.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qiskit==1.1.2\n",
      "  Using cached qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (0.15.1)\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (1.14.1)\n",
      "Requirement already satisfied: sympy>=1.3 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (1.13.1)\n",
      "Requirement already satisfied: dill>=0.3 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit==1.1.2) (2.8.2)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from qiskit==1.1.2) (4.9.0)\n",
      "Requirement already satisfied: symengine>=0.11 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from qiskit==1.1.2) (0.13.0)\n",
      "Requirement already satisfied: six>=1.5 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit==1.1.2) (1.16.0)\n",
      "Requirement already satisfied: pbr>=2.0.0 in /global/homes/j/junghoon/.local/perlmutter/python-3.11/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit==1.1.2) (6.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages (from sympy>=1.3->qiskit==1.1.2) (1.3.0)\n",
      "Using cached qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "Installing collected packages: qiskit\n",
      "  Attempting uninstall: qiskit\n",
      "    Found existing installation: qiskit 2.0.0\n",
      "    Uninstalling qiskit-2.0.0:\n",
      "      Successfully uninstalled qiskit-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "qiskit-ibm-runtime 0.37.0 requires qiskit>=1.4.1, but you have qiskit 1.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed qiskit-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install mitiq\n",
    "# !pip uninstall qiskit-terra -y\n",
    "# !pip uninstall qiskit-machine-learning -y\n",
    "# !pip uninstall qiskit-ibm-runtime -y\n",
    "# !pip uninstall qiskit_aer -y\n",
    "# !pip install qiskit-machine-learning qiskit-ibm-runtime qiskit-aer\n",
    "# !pip install qiskit==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42f10fa-1f25-4ca0-9741-04091d5d271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qiskit Version : 1.1.2\n",
      "Qiskit Machine Learning Version : 0.8.2\n",
      "Qiskit-IBM-runtime Version : 0.37.0\n",
      "Qiskit-Aer Version : 0.17.0\n",
      "Mitiq Version : 0.43.0\n"
     ]
    }
   ],
   "source": [
    "import qiskit\n",
    "import qiskit_machine_learning\n",
    "import qiskit_ibm_runtime\n",
    "import qiskit_aer\n",
    "import mitiq\n",
    "\n",
    "print('Qiskit Version :', qiskit.version.get_version_info())\n",
    "print('Qiskit Machine Learning Version :', qiskit_machine_learning.__version__)\n",
    "print('Qiskit-IBM-runtime Version :', qiskit_ibm_runtime.__version__)\n",
    "print('Qiskit-Aer Version :', qiskit_aer.__version__)\n",
    "print('Mitiq Version :', mitiq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23df9b9b-ee7b-4b0e-8989-f83792d7603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN, NeuralNetwork\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.utils import algorithm_globals\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit_ibm_runtime import Estimator as RuntimeEstimator\n",
    "# from qiskit.primitives import BackendEstimatorV2 as Estimator\n",
    "# from qiskit.primitives import Estimator\n",
    "\n",
    "from mitiq import zne\n",
    "from mitiq.interface.mitiq_qiskit.qiskit_utils import initialized_depolarizing_noise\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Optional, Tuple, Callable\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd38531-a90e-4981-a212-ae875e57e911",
   "metadata": {},
   "source": [
    "# IBM Quantum Hardware Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d23608b-c4e4-4131-b5ba-6c829693ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'cb4921703fe282148f320172517ac70528ebb67187d712333db2622503d323ff4e3680292cb77b69c8f344599281f9c4d43808b1d7478f1b01509b1c3017afa8'\n",
    "service = QiskitRuntimeService(\n",
    "    channel='ibm_quantum',\n",
    "    instance='yonsei-dedicated/internal/default',\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "# Or save your credentials on disk.\n",
    "# QiskitRuntimeService.save_account(channel='ibm_quantum', instance='yonsei-dedicated/internal/default', token='<IBM Quantum API key>')\n",
    "\n",
    "backend = service.least_busy(operational=True, simulator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cc0b57-0b28-4b9f-b0eb-635812e472ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cuda\n"
     ]
    }
   ],
   "source": [
    "# estimator = Estimator(backend=backend)\n",
    "# sampler = Sampler(backend)\n",
    "\n",
    "algorithm_globals.random_seed = 12345\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Running on \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b123b8-7406-47d7-83fe-9f71f9a0627f",
   "metadata": {},
   "source": [
    "# QCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1538a20-2f20-474c-907e-5cc764a407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(num_qubits: int, name: str = \"conv\") -> QuantumCircuit:\n",
    "    \"\"\"\n",
    "    Create a 'convolution' layer on `num_qubits`.\n",
    "    Fill in whatever parametric gates / entangling pattern\n",
    "    you like, sized to `num_qubits`.\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits, name=name)\n",
    "    # ------------------------------------------------\n",
    "    # Example (placeholder): a layer of Rx, Rz gates\n",
    "    # + ring entangling with CNOTs\n",
    "    # ------------------------------------------------\n",
    "    for q in range(num_qubits):\n",
    "        theta = Parameter(f\"θ_{name}_{q}\")  # Qiskit parameter\n",
    "        phi   = Parameter(f\"φ_{name}_{q}\")  # Qiskit parameter\n",
    "        qc.ry(theta, q)\n",
    "        qc.rz(phi,   q)\n",
    "    # Entangle in a ring, e.g. CNOT(q, q+1 mod num_qubits)\n",
    "    for q in range(num_qubits):\n",
    "        qc.cx(q, (q+1) % num_qubits)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "def pool_layer(left_qubits: list[int], right_qubits: list[int], name: str = \"pool\") -> QuantumCircuit:\n",
    "    \"\"\"\n",
    "    Pool pairs of qubits, one from `left_qubits` and one from `right_qubits`.\n",
    "    Typically merges 2 qubits -> 1 logical qubit by a controlled operation.\n",
    "\n",
    "    Note: The circuit you return must have a total 'width' equal to the\n",
    "    sum of len(left_qubits) + len(right_qubits).\n",
    "    \"\"\"\n",
    "    n_left = len(left_qubits)\n",
    "    n_right = len(right_qubits)\n",
    "    qc = QuantumCircuit(n_left + n_right, name=name)\n",
    "    # Simple example: controlled-Z from each left qubit to each right qubit\n",
    "    for i in range(min(n_left, n_right)):\n",
    "        qc.cz(i, n_left + i)\n",
    "\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d516cc71-c74c-4f64-88f9-3c44a8eef89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qcnn_circuit(\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    conv_layer_func=conv_layer,\n",
    "    pool_layer_func=pool_layer\n",
    "):\n",
    "    \"\"\"\n",
    "    Build the QCNN circuit (but do *not* wrap it in EstimatorQNN yet).\n",
    "    Returns (circuit, input_params, weight_params, observable).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Feature map\n",
    "    feature_map = ZFeatureMap(n_qubits)\n",
    "\n",
    "    # 2) Ansatz - the parametric part\n",
    "    ansatz = QuantumCircuit(n_qubits, name=\"Ansatz\")\n",
    "\n",
    "    active_qubits = list(range(n_qubits))\n",
    "    for layer_index in range(n_layers):\n",
    "        layer_name = layer_index + 1\n",
    "        # Convolution\n",
    "        conv_qc = conv_layer_func(len(active_qubits), f\"c{layer_name}\")\n",
    "        ansatz.compose(conv_qc, qubits=active_qubits, inplace=True)\n",
    "\n",
    "        # Pooling\n",
    "        half = len(active_qubits) // 2\n",
    "        left_block = active_qubits[:half]\n",
    "        right_block = active_qubits[half:]\n",
    "        pool_qc = pool_layer_func(left_block, right_block, f\"p{layer_name}\")\n",
    "        ansatz.compose(pool_qc, qubits=active_qubits, inplace=True)\n",
    "        active_qubits = right_block\n",
    "\n",
    "    # 3) Combine feature map + ansatz into one circuit\n",
    "    full_circuit = QuantumCircuit(n_qubits, name=\"QCNN\")\n",
    "    full_circuit.compose(feature_map, range(n_qubits), inplace=True)\n",
    "    full_circuit.compose(ansatz,      range(n_qubits), inplace=True)\n",
    "\n",
    "    # 4) Define observable (e.g., measure Z on first qubit)\n",
    "    obs_label = \"Z\" + \"I\" * (n_qubits - 1)\n",
    "    observable = SparsePauliOp.from_list([(obs_label, 1.0)])\n",
    "\n",
    "    # Identify which parameters belong to the feature map vs. ansatz\n",
    "    input_params  = list(feature_map.parameters)\n",
    "    weight_params = list(ansatz.parameters)\n",
    "\n",
    "    return full_circuit, input_params, weight_params, observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6faac08f-4641-4323-b679-6c923ddda13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuntimeNeuralNetwork(NeuralNetwork):\n",
    "    def __init__(self, circuit, observable, input_params, weight_params, backend_name):\n",
    "        super().__init__(\n",
    "            num_inputs=len(input_params),\n",
    "            num_weights=len(weight_params),\n",
    "            sparse=False,\n",
    "            output_shape=(1,),\n",
    "            input_gradients=False\n",
    "        )\n",
    "        self._circuit = circuit\n",
    "        self._observable = observable\n",
    "        self._input_params = input_params\n",
    "        self._weight_params = weight_params\n",
    "\n",
    "        self._estimator = RuntimeEstimator(\n",
    "            mode=backend_name\n",
    "        )\n",
    "\n",
    "    def _forward(self, input_data: np.ndarray, weights: np.ndarray):\n",
    "        batch_size = input_data.shape[0]\n",
    "        # input_data -> shape (batch_size, num_inputs)\n",
    "        input_data = np.array(input_data, dtype=float).reshape(batch_size, -1)\n",
    "        weights = np.array(weights, dtype=float).flatten()  # shape (num_weights,)\n",
    "        # Build a list of param_values for each item in the batch\n",
    "        param_values_list = []\n",
    "        for b in range(batch_size):\n",
    "            param_values = np.concatenate([input_data[b], weights])\n",
    "            param_values_list.append(param_values.tolist())\n",
    "        # Pass them all in one call\n",
    "        pubs = []\n",
    "        for pvals in param_values_list:\n",
    "            pubs.append((self._circuit, self._observable, pvals))\n",
    "        job = self._estimator.run(pubs)\n",
    "        res = job.result()\n",
    "\n",
    "        outputs = []\n",
    "        for pub_res in res:\n",
    "            # pub_res is a PubResult\n",
    "            val = float(pub_res.data.evs)  # shape=() => single scalar\n",
    "            outputs.append(val)\n",
    "\n",
    "        outputs = np.array(outputs).reshape((batch_size, 1))\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _backward(self, input_data, weights):\n",
    "        batch_size = input_data.shape[0]\n",
    "        # Return dummy zeros (no analytic gradient).\n",
    "        d_input = np.zeros((batch_size, self._num_inputs), dtype=float)\n",
    "        d_weights = np.zeros((batch_size, self._num_weights), dtype=float)\n",
    "        return d_input, d_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bce4fe3c-c8b7-4ac2-8258-6d0947f4ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_qubits(circuit):\n",
    "    \"\"\"Create a new circuit with only the qubits actually used by instructions.\"\"\"\n",
    "    used_qubits = set()\n",
    "    for instr, qbs, cbs in circuit.data:\n",
    "        used_qubits.update(qbs)\n",
    "\n",
    "    # Sort by the actual index in this circuit\n",
    "    used_qubits = sorted(used_qubits, key=lambda qb: circuit.find_bit(qb).index)\n",
    "\n",
    "    # Map old qubit objects -> new indices [0..(len-1)]\n",
    "    qubit_map = {qb: i for i, qb in enumerate(used_qubits)}\n",
    "\n",
    "    # Build a new circuit with fewer qubits\n",
    "    new_circ = QuantumCircuit(len(used_qubits), name=circuit.name)\n",
    "    for instr, qbs, cbs in circuit.data:\n",
    "        new_qbs = [qubit_map[qb] for qb in qbs]\n",
    "        new_circ.append(instr, new_qbs, cbs)\n",
    "\n",
    "    # Copy over metadata if desired\n",
    "    new_circ.metadata = circuit.metadata\n",
    "    return new_circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "823eaf2f-deda-4770-b667-687762284ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridQCNN(nn.Module):\n",
    "    def __init__(self, n_qubits=8, n_layers=2, input_dim=784, backend=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # A classical front-end to reduce 784 -> n_qubits\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_qubits)\n",
    "        )\n",
    "\n",
    "        # 1) Build the raw QCNN circuit + param metadata\n",
    "        raw_circuit, input_params, weight_params, observable = build_qcnn_circuit(\n",
    "            n_qubits=n_qubits,\n",
    "            n_layers=n_layers,\n",
    "            conv_layer_func=conv_layer,\n",
    "            pool_layer_func=pool_layer\n",
    "        )\n",
    "\n",
    "        # 2) Transpile the circuit for the chosen backend\n",
    "        #    If you're using Aer locally, pass backend=Aer.get_backend('aer_simulator')\n",
    "        #    If you're using IBM hardware, pass backend=service.backend(\"ibmq_lima\"), etc.\n",
    "        my_layout = qiskit.transpiler.Layout({raw_circuit.qubits[i]: i for i in range(6)})\n",
    "        transpiled_circuit = transpile(raw_circuit, backend=backend,\n",
    "                                       initial_layout=my_layout, layout_method=\"sabre\",      # or \"trivial\", \"dense\"\n",
    "                                       routing_method=\"sabre\",\n",
    "                                       optimization_level=3,)\n",
    "        reduced_circuit = reduce_qubits(transpiled_circuit)\n",
    "\n",
    "        # 3) Build the QNN with the transpiled circuit\n",
    "        qnn = RuntimeNeuralNetwork(\n",
    "            circuit=reduced_circuit,\n",
    "            observable=observable,\n",
    "            input_params=input_params,\n",
    "            weight_params=weight_params,\n",
    "            backend_name=backend\n",
    "        )\n",
    "\n",
    "        # 4) Wrap with TorchConnector => a PyTorch module\n",
    "        self.qnn = TorchConnector(qnn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, 784) for MNIST\n",
    "        \"\"\"\n",
    "        # Classical part\n",
    "        reduced_x = self.fc(x)\n",
    "\n",
    "        # Quantum part => outputs shape (batch_size, 1)\n",
    "        output = self.qnn(reduced_x)\n",
    "\n",
    "        # Squeeze the last dim to get (batch_size,)\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71297423-f261-4fc5-9513-ec0b1ffb67df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425a5c26-bb72-47ab-a8c3-5fab3272c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_binary(seed, n_train, n_valtest, device, batch_size, classes=(0, 1)):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Load dataset with transformation\n",
    "    transform = Compose([ToTensor(), lambda x: x.view(-1)])  # Flatten MNIST images\n",
    "    data_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    data_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    input_dim = 28 * 28\n",
    "\n",
    "    # Filter for binary classes\n",
    "    train_mask = (data_train.targets == classes[0]) | (data_train.targets == classes[1])\n",
    "    test_mask = (data_test.targets == classes[0]) | (data_test.targets == classes[1])\n",
    "    X_train = data_train.data[train_mask].float() / 255.0  # Normalize pixel values to [0, 1]\n",
    "    y_train = data_train.targets[train_mask].clone().detach()\n",
    "    X_test = data_test.data[test_mask].float() / 255.0\n",
    "    y_test = data_test.targets[test_mask].clone().detach()\n",
    "\n",
    "    # Binarize labels\n",
    "    y_train = (y_train == classes[1]).long()\n",
    "    y_test = (y_test == classes[1]).long()\n",
    "\n",
    "    # Shuffle data\n",
    "    shuffle_idx = torch.randperm(len(y_train))\n",
    "    X_train = X_train[shuffle_idx]\n",
    "    y_train = y_train[shuffle_idx]\n",
    "\n",
    "    shuffle_idx2 = torch.randperm(len(y_test))\n",
    "    X_test = X_test[shuffle_idx2]\n",
    "    y_test = y_test[shuffle_idx2]\n",
    "\n",
    "    # Flatten images\n",
    "    X_train = X_train.view(-1, 28*28)\n",
    "    X_test = X_test.view(-1, 28*28)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_X = X_train.to(device)\n",
    "    train_y = y_train.to(device)\n",
    "    test_X = X_test.to(device)\n",
    "    test_y = y_test.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    valtest_dataset = TensorDataset(test_X, test_y)\n",
    "\n",
    "    # Equally split validation and test sets\n",
    "    val_size = int(0.5 * len(valtest_dataset))\n",
    "    test_size = len(valtest_dataset) - val_size\n",
    "    val_dataset, test_dataset = random_split(valtest_dataset, [val_size, test_size])\n",
    "\n",
    "    # DataLoader parameters\n",
    "    params = {'shuffle': True, 'batch_size': batch_size} if batch_size > 0 else {'shuffle': True}\n",
    "    test_params = {'shuffle': False, 'batch_size': batch_size} if batch_size > 0 else {'shuffle': False}\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, **params)\n",
    "    val_loader = DataLoader(val_dataset, **test_params)\n",
    "    test_loader = DataLoader(test_dataset, **test_params)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3beb6d-eb1d-47ec-ae6b-6545d6605fec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c1b1a32-30d7-4023-8e17-3c77c691f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Calculate Running Time ########################################\n",
    "def epoch_time(start_time: float, end_time: float) -> Tuple[float, float]:\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "################################# Performance & Density Matrices ################################\n",
    "# Training loop\n",
    "def train_perf(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Ensure that data is on the same device (GPU or CPU)\n",
    "        labels = labels.float()   # Ensure labels are of type float for BCEWithLogitsLoss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Collect labels and outputs for AUROC\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())       \n",
    "        \n",
    "    # Calculate train AUROC\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    train_auroc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    return train_loss / len(dataloader), train_auroc\n",
    "\n",
    "\n",
    "# Validation/Test loop\n",
    "def evaluate_perf(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Ensure that data is on the same device (GPU or CPU)\n",
    "            labels = labels.float()   # Ensure labels are of type float for BCEWithLogitsLoss\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Collect labels and outputs for AUROC\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    auroc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    return running_loss / len(dataloader), auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "094c5f54-4fb1-4ce4-9b99-41afe2d6d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantumCNN_run(n_qubits, n_layers, input_dim, num_epochs):\n",
    "    print(\"Running on \", device)\n",
    "    model = HybridQCNN(n_qubits, n_layers, input_dim, backend).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "    # criterion = nn.CrossEntropyLoss()   # Loss function for multi-class classification\n",
    "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4, eps=1e-8)\n",
    "    # optimizer = COBYLA(maxiter=200)\n",
    "        \n",
    "    # Training process\n",
    "    train_metrics, valid_metrics, test_metrics = [], [], []\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_auc = train_perf(model, train_loader, optimizer, criterion)\n",
    "        train_metrics.append({'epoch': epoch + 1, 'train_loss': train_loss, 'train_auc': train_auc})    \n",
    "    \n",
    "        valid_loss, valid_auc = evaluate_perf(model, val_loader, criterion)\n",
    "        valid_metrics.append({'epoch': epoch + 1, 'valid_loss': valid_loss, 'valid_auc': valid_auc})\n",
    "    \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, AUC: {train_auc:.4f} | Validation Loss: {valid_loss:.4f}, AUC: {valid_auc:.4f}\")\n",
    "\n",
    "    # Final evaluation on the test set\n",
    "    test_loss, test_auc = evaluate_perf(model, test_loader, criterion)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, AUC: {test_auc:.4f}\")\n",
    "    test_metrics.append({'epoch': num_epochs, 'test_loss': test_loss, 'test_auc': test_auc}) \n",
    "\n",
    "    # Combine all metrics into a pandas DataFrame\n",
    "    metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_metrics[epoch]['train_loss'],\n",
    "            'train_auc': train_metrics[epoch]['train_auc'],\n",
    "            'valid_loss': valid_metrics[epoch]['valid_loss'],\n",
    "            'valid_auc': valid_metrics[epoch]['valid_auc'],\n",
    "            'test_loss': test_metrics[0]['test_loss'],\n",
    "            'test_auc': test_metrics[0]['test_auc'],\n",
    "        })\n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    # Save to CSV\n",
    "    # csv_filename = f\"QuantumCNN_performance.csv\"\n",
    "    # metrics_df.to_csv(csv_filename, index=False)\n",
    "    # print(f\"Metrics saved to {csv_filename}\")\n",
    "        \n",
    "    return test_loss, test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d49d3d-ec01-4e37-95b4-4c7620526338",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baf1e77a-6c76-4a39-9f0e-c83e4eebac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, input_dim = load_mnist_binary(seed=2025, n_train=70, n_valtest=30, device=device, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c288922-42b7-4356-a6d8-37f9da85d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/396 [2:43:42<357:25:19, 3274.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mQuantumCNN_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36mQuantumCNN_run\u001b[0;34m(n_qubits, n_layers, input_dim, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     13\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m     train_loss, train_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_perf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     train_metrics\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: train_auc})    \n\u001b[1;32m     18\u001b[0m     valid_loss, valid_auc \u001b[38;5;241m=\u001b[39m evaluate_perf(model, val_loader, criterion)\n",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m, in \u001b[0;36mtrain_perf\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()   \u001b[38;5;66;03m# Ensure labels are of type float for BCEWithLogitsLoss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[48], line 50\u001b[0m, in \u001b[0;36mHybridQCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m reduced_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Quantum part => outputs shape (batch_size, 1)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Squeeze the last dim to get (batch_size,)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/qiskit_machine_learning/connectors/torch_connector.py:423\u001b[0m, in \u001b[0;36mTorchConnector.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     input_ \u001b[38;5;241m=\u001b[39m input_data\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TorchNNFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_neural_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/qiskit_machine_learning/connectors/torch_connector.py:164\u001b[0m, in \u001b[0;36m_TorchNNFunction.forward\u001b[0;34m(ctx, input_data, weights, neural_network, sparse)\u001b[0m\n\u001b[1;32m    161\u001b[0m ctx\u001b[38;5;241m.\u001b[39msparse \u001b[38;5;241m=\u001b[39m sparse\n\u001b[1;32m    162\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(input_data, weights)\n\u001b[0;32m--> 164\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msparse:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m neural_network\u001b[38;5;241m.\u001b[39msparse:\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/qiskit_machine_learning/neural_networks/neural_network.py:229\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m    227\u001b[0m input_, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(input_data)\n\u001b[1;32m    228\u001b[0m weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_weights(weights)\n\u001b[0;32m--> 229\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_forward_output(output_data, shape)\n",
      "Cell \u001b[0;32mIn[46], line 34\u001b[0m, in \u001b[0;36mRuntimeNeuralNetwork._forward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m     32\u001b[0m     pubs\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_circuit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observable, pvals))\n\u001b[1;32m     33\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator\u001b[38;5;241m.\u001b[39mrun(pubs)\n\u001b[0;32m---> 34\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pub_res \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# pub_res is a PubResult\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/qiskit_ibm_runtime/runtime_job_v2.py:132\u001b[0m, in \u001b[0;36mRuntimeJobV2.result\u001b[0;34m(self, timeout, decoder)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the results of the job.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    RuntimeInvalidStateError: If the job was cancelled, and attempting to retrieve result.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m _decoder \u001b[38;5;241m=\u001b[39m decoder \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_result_decoder\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_final_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reason \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reason \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/qiskit_ibm_runtime/runtime_job_v2.py:251\u001b[0m, in \u001b[0;36mRuntimeJobV2.wait_for_final_state\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m timeout:\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RuntimeJobTimeoutError(\n\u001b[1;32m    249\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out waiting for job to complete after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m secs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m             )\n\u001b[0;32m--> 251\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    252\u001b[0m         status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mTimeoutError:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "QuantumCNN_run(n_qubits=6, n_layers=2, input_dim=input_dim, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf8be6-74a6-4b99-bd0f-e87d0dcdc8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77febcf7-781a-491f-b7bf-1fa591e584c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42f962a-24ad-4ec4-993c-3ae75b58d515",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ZNE Error Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558037cc-38fa-444b-b47b-b8e69b4b8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridQCNN(nn.Module):\n",
    "    def __init__(self, n_qubits=8, n_layers=2, input_dim=784):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 2, kernel_size=5)\n",
    "        # self.conv2 = nn.Conv2d(2, 16, kernel_size=5)\n",
    "        # self.dropout = nn.Dropout2d()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_qubits)\n",
    "        )\n",
    "        qnn = build_qcnn(n_qubits=n_qubits, n_layers=n_layers, estimator=estimator)\n",
    "        zne.execute_with_zne(circuit, ibmq_executor)\n",
    "        self.qnn = TorchConnector(qnn)  # Apply torch connector, weights chosen\n",
    "        # uniformly at random from interval [-1,1].\n",
    "        # self.fc2 = nn.Linear(1, 1)  # 1-dimensional output from QNN\n",
    "\n",
    "    def forward(self, x):\n",
    "        reduced_x = self.fc(x)\n",
    "        output = self.qnn(reduced_x)  # apply QNN\n",
    "        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c6401-7e3d-4181-b4d6-d1331ca130b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b525671-3338-4f81-ad8d-07756c2e56f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
